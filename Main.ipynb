{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import math\n",
    "import os\n",
    "import nethook\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def parse_line(line):\n",
    "        tokens = [\n",
    "            token for token in line.split(' ')\n",
    "            if token not in ['', '']\n",
    "        ]\n",
    "        if len(tokens) == 0:\n",
    "            return None\n",
    "        spaces = [True for _ in range(len(tokens)-1)] + [False]\n",
    "        assert len(tokens) == len(spaces), f\"{len(tokens)} != {len(spaces)}\"\n",
    "\n",
    "        doc = spacy.tokens.doc.Doc(\n",
    "            nlp.vocab, words=tokens, spaces=spaces)\n",
    "        for name, proc in nlp.pipeline:\n",
    "            doc = proc(doc)\n",
    "        return [str(sent) for sent in doc.sents]\n",
    "\n",
    "\n",
    "def parse_data_file(data_file, max_sentences, pool,shuffle=False):\n",
    "    data_file = data_file\n",
    "    \n",
    "    multiprocess = 20\n",
    "\n",
    "    parsed = []\n",
    "    with open(data_file, \"r\") as fd:\n",
    "        lines = fd.readlines()\n",
    "    if shuffle:\n",
    "        random.seed(0xdead)\n",
    "        random.shuffle(lines)\n",
    "    \n",
    "    \n",
    "    \n",
    "    window = 5\n",
    "    lines2 = []\n",
    "    for i in range(0,len(lines)-window):\n",
    "        line = lines[i:i+window]\n",
    "        lines2.append(\"\".join(line))\n",
    "    lines = lines2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    max_sentences = max_sentences\n",
    "    # max_sentences = max_sentences\n",
    "    \n",
    "    if max_sentences > -1:\n",
    "        line_it = pool.imap_unordered(parse_line, lines)\n",
    "        sentence_pb = tqdm(total=max_sentences)\n",
    "    else:\n",
    "        line_it = pool.imap_unordered(parse_line, lines)\n",
    "\n",
    "    for curr_sentences in line_it:\n",
    "        if curr_sentences == None:\n",
    "            continue\n",
    "        if -1 < max_sentences:\n",
    "            sentence_pb.update(len(curr_sentences))\n",
    "        parsed.extend(curr_sentences)\n",
    "        if -1 < max_sentences <= len(parsed):\n",
    "            parsed = parsed[:max_sentences]\n",
    "            # pool.terminate()\n",
    "            break\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 477/500 [01:03<00:04,  5.59it/s]"
     ]
    }
   ],
   "source": [
    "parsed = []\n",
    "def get_files(path):\n",
    "    files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            files.append(os.path.join(dirpath, filename))\n",
    "    return files\n",
    "\n",
    "\n",
    "pool = mp.Pool(20)\n",
    "files1 = get_files('/u/amo-d1/grad/mha361/work/Code-LMs/Data/Code/Python/')\n",
    "files2 = get_files('/u/amo-d1/grad/mha361/work/Code-LMs/Data/Code/Java/')\n",
    "\n",
    "files = files1[:500]\n",
    "# files.extend(files2[:250])\n",
    "\n",
    "random.shuffle(files)\n",
    "for file in tqdm(files):\n",
    "    p = parse_data_file(file,-1,pool)\n",
    "    parsed.extend(p)\n",
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null) {\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parsed)\n",
    "parsed[5000]\n",
    "# for i in parsed:\n",
    "#     print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from packaging import version\n",
    "assert version.parse(transformers.__version__) >= version.parse(\"4.23.0\")\n",
    "\n",
    "tokenizer_polycoder = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-mono\")\n",
    "model_polycoder = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-2B-mono\")\n",
    "\n",
    "\n",
    "\n",
    "model_polycoder = model_polycoder.to(\"cuda\")\n",
    "tokenizer_polycoder.pad_token = tokenizer_polycoder.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_polycoder.pad_token = tokenizer_polycoder.eos_token\n",
    "def _build_batches(parsed, batch_size):\n",
    "    \n",
    "    for i in range(0,math.ceil(len(parsed)/batch_size)):\n",
    "        \n",
    "        to = min(len(parsed),batch_size*(i+1))\n",
    "\n",
    "        yield tokenizer_polycoder(parsed[batch_size*i:to],padding=True, truncation=True,max_length=200, return_tensors=\"pt\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _aggregate_layer_values(all_values, batch_index, start_idx, seq_len):\n",
    "    max_layer_vals = []\n",
    "    max_layer_pos = []\n",
    "    \n",
    "    if seq_len > 0:\n",
    "        for layer_vals in all_values:\n",
    "            effective_layer_vals = layer_vals[batch_index][start_idx:start_idx + seq_len]\n",
    "            \n",
    "            max_vals = effective_layer_vals.max(axis=0)\n",
    "\n",
    "            max_layer_vals.append(max_vals[0].cpu().detach().numpy().tolist())\n",
    "            max_layer_pos.append(max_vals[1].cpu().detach().numpy().tolist())\n",
    "\n",
    "            #For the sparsity experiment: we use a randomly chosen position for all layers\n",
    "    return max_layer_vals, max_layer_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def score(parsed1):\n",
    "    batch_size = 3\n",
    "    l = []\n",
    "    results = []\n",
    "    for i in range(0,32):\n",
    "        l.append('gpt_neox.layers.'+str(i)+'.mlp.dense_h_to_4h')\n",
    "    with nethook.TraceDict(model_polycoder, l) as ret:\n",
    "        for batch in _build_batches(parsed1,batch_size):#,total=math.ceil(len(parsed)/batch_size)):\n",
    "            torch.cuda.empty_cache()\n",
    "            # torch.cuda.synchronize(device=\"cuda\")\n",
    "            batch = batch.to(\"cuda\")\n",
    "            net_input = batch['input_ids']\n",
    "            out = model_polycoder(net_input)\n",
    "            \n",
    "            fc1_vals = [\n",
    "                        ret[layer_fc1_vals].output#.transpose(0, 1)//works without transpose somehow\n",
    "                        for layer_fc1_vals in ret\n",
    "                    ]\n",
    "            # print(len(fc1_vals[0]))\n",
    "            \n",
    "            \n",
    "            \n",
    "            bsz = len(fc1_vals[0])\n",
    "            hypos = []\n",
    "            start_idxs = 0\n",
    "            for i in range(0, bsz):\n",
    "                max_layer_fc1_vals_i, max_pos_layer_fc1_vals_i  = _aggregate_layer_values(all_values = fc1_vals, batch_index = i, start_idx = start_idxs, seq_len = torch.count_nonzero(batch['input_ids'][0]))\n",
    "                hypos.append({\n",
    "                            'max_fc1_vals': max_layer_fc1_vals_i,\n",
    "                            'max_pos_fc1_vals': max_pos_layer_fc1_vals_i,\n",
    "                        })\n",
    "            results.extend(hypos)#didnt sort on input ids\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def format_ffn_values(hypos, sentences, pos_neg, extract_mode, output_values_shape=False):\n",
    "    for i, hypo in enumerate(hypos):\n",
    "        if i == 0 and output_values_shape:\n",
    "            yield (len(hypo['max_fc1_vals']), len(hypo['max_fc1_vals'][0]))\n",
    "\n",
    "        if extract_mode == \"layer-raw\":\n",
    "            yield {\n",
    "                'pos_neg': pos_neg,\n",
    "                'text': str(sentences[i]),\n",
    "                'max_fc1_vals': hypo['max_fc1_vals'],\n",
    "                'max_pos_fc1_vals': hypo['max_pos_fc1_vals'],\n",
    "            }\n",
    "        elif extract_mode == \"dim\":\n",
    "            yield json.dumps({\n",
    "                'pos_neg': pos_neg,\n",
    "                'text': str(sentences[i]),\n",
    "                'output_dist_vals': hypo['output_dist_vals'],\n",
    "                'output_dist_conf': hypo['output_dist_conf'],\n",
    "                'residual_ffn_output_rank': hypo['residual_ffn_output_rank'],\n",
    "                'residual_ffn_output_prob': hypo['residual_ffn_output_prob'],\n",
    "                'residual_ffn_argmax': hypo['residual_ffn_argmax'],\n",
    "                'residual_ffn_argmax_prob': hypo['residual_ffn_argmax_prob'],\n",
    "                'ffn_residual_output_rank': hypo['ffn_residual_output_rank'],\n",
    "                'ffn_residual_output_prob': hypo['ffn_residual_output_prob'],\n",
    "                'dim_pattern_preds': hypo['dim_pattern_preds'],\n",
    "                'dim_pattern_output_rank': hypo['dim_pattern_output_rank'],\n",
    "                'dim_pattern_ffn_output_rank': hypo['dim_pattern_ffn_output_rank'],\n",
    "                'dim_pattern_ffn_output_prob': hypo['dim_pattern_ffn_output_prob'],\n",
    "                'coeffs_vals': hypo['coeffs_vals'],\n",
    "                'coeffs_l0': hypo['coeffs_l0'],\n",
    "                'coeffs_residual_rank': hypo['coeffs_residual_rank'],\n",
    "                'random_pos': hypo['random_pos'],\n",
    "            }) + '\\n'\n",
    "        else:\n",
    "            assert extract_mode == \"layer\"\n",
    "            yield json.dumps({\n",
    "                'pos_neg': pos_neg,\n",
    "                'text': str(sentences[i]),\n",
    "                'output_dist_vals': hypo['output_dist_vals'],\n",
    "                'layer_output_argmax': hypo['layer_output_argmax'],\n",
    "                'layer_output_argmax_prob': hypo['layer_output_argmax_prob'],\n",
    "                'residual_argmax': hypo['residual_argmax'],\n",
    "                'residual_argmax_prob': hypo['residual_argmax_prob'],\n",
    "                'residual_argmax_change': hypo['residual_argmax_change'],\n",
    "                'residual_output_rank': hypo['residual_output_rank'],\n",
    "                'residual_output_prob': hypo['residual_output_prob'],\n",
    "                'ffn_matching_dims_count': hypo['ffn_matching_dims_count'],\n",
    "                'ffn_output_rank': hypo['ffn_output_rank'],\n",
    "                'ffn_output_prob': hypo['ffn_output_prob'],\n",
    "                'ffn_residual_output_rank': hypo['ffn_residual_output_rank'],\n",
    "                'ffn_residual_output_prob': hypo['ffn_residual_output_prob'],\n",
    "                'ffn_argmax': hypo['ffn_argmax'],\n",
    "                'ffn_argmax_prob': hypo['ffn_argmax_prob'],\n",
    "                'coeffs_l0': hypo['coeffs_l0'],\n",
    "                'coeffs_residual_rank': hypo['coeffs_residual_rank'],\n",
    "                'random_pos': hypo['random_pos'],\n",
    "            }) + '\\n'\n",
    "\n",
    "\n",
    "def extract_ffn_info(all_ffn_values, extract_mode, output_file):\n",
    "    print(\"load extracted values...\")\n",
    "    records = []\n",
    "    skip_count = 0\n",
    "    for i, ffn_vals in tqdm(enumerate(all_ffn_values)):\n",
    "        loaded_vals = json.loads(ffn_vals)\n",
    "        # random_pos = loaded_vals['random_pos']\n",
    "        # if random_pos == -1:\n",
    "        #     skip_count += 1\n",
    "        #     continue\n",
    "        records.append(loaded_vals)\n",
    "\n",
    "    print(f\"skipped {skip_count} examples.\")\n",
    "\n",
    "    # store as dataframe\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    if extract_mode == \"dim\":\n",
    "        df = df[\n",
    "            [col for col in df.columns\n",
    "             if col in [\n",
    "                 'text', 'output_dist_vals', 'coeffs_vals', 'residual_ffn_output_rank', 'residual_ffn_output_prob',\n",
    "                 'residual_output_prob', 'residual_ffn_argmax', 'residual_ffn_argmax_prob',\n",
    "                 'ffn_residual_output_rank', 'ffn_residual_output_prob',\n",
    "                 'dim_pattern_preds', 'dim_pattern_output_rank',\n",
    "                 'dim_pattern_ffn_output_rank', 'dim_pattern_ffn_output_prob', 'random_pos']\n",
    "             ]\n",
    "        ]\n",
    "    else:\n",
    "        assert extract_mode == \"layer\"\n",
    "        df = df[\n",
    "            [col for col in df.columns\n",
    "             if col in [\n",
    "                 'text', 'output_dist_vals', 'layer_output_argmax', 'layer_output_argmax_prob',\n",
    "                 'residual_argmax', 'residual_argmax_prob', 'residual_argmax_change',\n",
    "                 'residual_output_rank', 'residual_output_prob', 'ffn_matching_dims_count',\n",
    "                 'ffn_output_rank', 'ffn_output_prob', 'ffn_residual_output_rank', 'ffn_residual_output_prob',\n",
    "                 'ffn_argmax', 'ffn_argmax_prob',\n",
    "                 'coeffs_l0', 'coeffs_residual_rank', 'random_pos']\n",
    "             ]\n",
    "        ]\n",
    "    df.to_pickle(output_file)\n",
    "\n",
    "\n",
    "def get_trigger_examples(all_ffn_values, dims_for_analysis, num_sentences, values_shape, output_file,\n",
    "                         top_k=5, apply_relu=True, num_layers=32):\n",
    "    values_key = 'max_fc1_vals'\n",
    "    position_key = 'max_pos_fc1_vals'\n",
    "    hidden_size = values_shape[1]  # shape: (num_layers, hidden_size)\n",
    "    # if args.dims_for_analysis is not None and len(args.dims_for_analysis) > 0:\n",
    "    #     assert len([\n",
    "    #         dim for dim in dims_for_analysis\n",
    "    #         if dim < 0 or dim >= hidden_size\n",
    "    #     ]) == 0\n",
    "    # else:\n",
    "    dims_for_analysis = list(range(hidden_size))\n",
    "    layers = list(range(num_layers))\n",
    "    num_dims = len(dims_for_analysis)\n",
    "\n",
    "    layer_vals = np.zeros((num_layers, top_k, num_dims))\n",
    "    min_layer_vals_i = np.zeros((num_layers, num_dims), dtype=int)\n",
    "    token_indices = np.zeros((num_layers, top_k, num_dims))  # token indices\n",
    "    sentence_indices = np.zeros((num_layers, top_k, num_dims), dtype=int)\n",
    "    all_ffn_vals = []\n",
    "    j = 0\n",
    "    for i, ffn_vals in enumerate(tqdm(all_ffn_values, total=num_sentences)):\n",
    "        j+=1\n",
    "        loaded_vals = ffn_vals\n",
    "        val = loaded_vals.pop(values_key)\n",
    "        val_pos = loaded_vals.pop(position_key)\n",
    "        for layer_index in layers:\n",
    "            for d_i, d in enumerate(dims_for_analysis):\n",
    "                loc_ind = min_layer_vals_i[layer_index, d_i]\n",
    "                if val[layer_index][d] > layer_vals[layer_index, loc_ind, d_i]:\n",
    "                    layer_vals[layer_index, loc_ind, d_i] = val[layer_index][d]\n",
    "                    token_indices[layer_index, loc_ind, d_i] = val_pos[layer_index][d]\n",
    "                    sentence_indices[layer_index, loc_ind, d_i] = i\n",
    "                    min_layer_vals_i[layer_index, d_i] = np.argmin(\n",
    "                        layer_vals[layer_index, :, d_i])\n",
    "        all_ffn_vals.append(loaded_vals)\n",
    "    top_vals_per_dim = []\n",
    "    for layer_index in layers:\n",
    "        if apply_relu:\n",
    "            layer_vals[layer_index] = np.maximum(layer_vals[layer_index], 0)\n",
    "        else:\n",
    "            layer_vals[layer_index] = layer_vals[layer_index]\n",
    "        top_vals_per_dim.append(np.argsort(layer_vals[layer_index], axis=0)[-top_k::][::-1, :].T)\n",
    "\n",
    "    # write output\n",
    "    with open(output_file, \"w\") as fd:\n",
    "        for dim_i, dim in enumerate(dims_for_analysis):\n",
    "            dim_outputs = []\n",
    "            for layer_index in layers:\n",
    "                layer_output = []\n",
    "                for rank, i in enumerate(top_vals_per_dim[layer_index][dim_i]):\n",
    "                    layer_output.append({\n",
    "                        \"rank\": rank, 'layer_index': layer_index,\n",
    "                        'token_indice': token_indices[layer_index][i][dim_i],\n",
    "                        \"fc1_value\": layer_vals[layer_index][i][dim_i],\n",
    "                        \"text\": all_ffn_vals[sentence_indices[layer_index][i][dim_i]]['text']\n",
    "                    })\n",
    "                dim_outputs.append(layer_output)\n",
    "            fd.write(json.dumps({\"dim\": dim, \"top_values\": dim_outputs}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [4:29:07<00:00, 110.60s/it]  \n",
      "100%|██████████| 145046/145046 [4:28:27<00:00,  9.01it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_hypos():\n",
    "        for batch_i in tqdm(list(range(0, len(parsed), 1000))):\n",
    "            for hypo_parsed in score(\n",
    "                    parsed[batch_i:min(len(parsed), batch_i + 1000)]\n",
    "            ):\n",
    "                yield hypo_parsed\n",
    "\n",
    "all_ffn_values = format_ffn_values(hypos=get_hypos(),\n",
    "                                   sentences=parsed,\n",
    "                                   pos_neg=1,\n",
    "                                   extract_mode=\"layer-raw\",\n",
    "                                   output_values_shape=True)\n",
    "values_shape = next(all_ffn_values)\n",
    "\n",
    "get_trigger_examples(all_ffn_values,\n",
    "                    dims_for_analysis=22,\n",
    "                    num_sentences=len(parsed),\n",
    "                    values_shape=values_shape,\n",
    "                    output_file=\"top50_java_python_max_len200_single.jsonl\",\n",
    "                    top_k=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d1229a95fcbafab6ef03ba2e74295bf4b93531e79c6294ccaf0fb6e21a20ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
